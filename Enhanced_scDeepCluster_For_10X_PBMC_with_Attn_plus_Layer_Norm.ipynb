{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e53af6",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38911249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math, os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('./Baseline_Code')\n",
    "sys.path.append('./EnhancedscDeepCluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3b303c-8f5d-4703-8297-489cd88226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scDeepCluster\n",
    "from scDeepCluster import scDeepCluster\n",
    "from single_cell_tools import *\n",
    "from preprocess import read_dataset, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a532a6-d597-4cef-bb06-68648c1a72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModifiedscDeepCluster\n",
    "import model\n",
    "import model_single_cell_tools\n",
    "import model_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60af52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f879130030>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for repeatability\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc9d56",
   "metadata": {},
   "source": [
    "Setup parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e62117",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter setting\n",
    "'''\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.n_clusters = 8\n",
    "        self.knn = 20\n",
    "        self.resolution = 0.8\n",
    "        self.select_genes = 0\n",
    "        self.batch_size = 256\n",
    "        self.data_file = '10X_PBMC.h5'\n",
    "        self.maxiter = 2000\n",
    "        self.pretrain_epochs = 300\n",
    "        self.gamma = 1.\n",
    "        self.sigma = 2.5\n",
    "        self.update_interval = 1\n",
    "        self.tol = 0.001\n",
    "        self.use_attention = True\n",
    "        self.use_layernorm = True\n",
    "        self.use_batchnorm = False\n",
    "        # self.ae_weights = None\n",
    "        self.ae_weights = '10xPBMC_with_attn_plus_layer_weights.pth.tar'\n",
    "        self.save_dir = 'results/EnhancedscDeepCluster/'\n",
    "        self.ae_weight_file = '10xPBMC_with_attn_plus_layer_weights.pth.tar'\n",
    "        self.final_latent_file = '10xPBMC_final_latent_file.txt'\n",
    "        self.predict_label_file = '10xPBMC_pred_labels.txt'\n",
    "        self.device = 'cuda'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590d2b",
   "metadata": {},
   "source": [
    "Normalizating and preprocessing count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1ae96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 16653 genes and 4271 cells.\n",
      "<__main__.Args object at 0x000001F87F713820>\n",
      "(4271, 16653)\n",
      "(4271,)\n"
     ]
    }
   ],
   "source": [
    "data_mat = h5py.File(args.data_file)\n",
    "x = np.array(data_mat['X'])\n",
    "# y is the ground truth labels for evaluating clustering performance\n",
    "# If not existing, we skip calculating the clustering performance metrics (e.g. NMI ARI)\n",
    "if 'Y' in data_mat:\n",
    "    y = np.array(data_mat['Y'])\n",
    "else:\n",
    "    y = None\n",
    "data_mat.close()\n",
    "\n",
    "if args.select_genes > 0:\n",
    "    importantGenes = model_single_cell_tools.geneSelection(x, n=args.select_genes, plot=False)\n",
    "    x = x[:, importantGenes]\n",
    "\n",
    "# preprocessing scRNA-seq read counts matrix\n",
    "adata = sc.AnnData(x)\n",
    "if y is not None:\n",
    "    adata.obs['Group'] = y\n",
    "\n",
    "adata = model_preprocess.read_dataset(adata,\n",
    "                 transpose=False,\n",
    "                 test_split=False,\n",
    "                 copy=True)\n",
    "\n",
    "adata = model_preprocess.normalize(adata,\n",
    "                  size_factors=True,\n",
    "                  normalize_input=True,\n",
    "                  logtrans_input=True)\n",
    "\n",
    "input_size = adata.n_vars\n",
    "\n",
    "print(args)\n",
    "\n",
    "print(adata.X.shape)\n",
    "if y is not None:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827573d",
   "metadata": {},
   "source": [
    "Build scDeepCluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d2f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedscDeepCluster(\n",
      "  (dropoutLayer): Dropout(p=0.8, inplace=False)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=16653, out_features=256, bias=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (attention): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.EnhancedscDeepCluster(input_dim=adata.n_vars, z_dim=32, use_attention=args.use_attention, use_layernorm=args.use_layernorm, use_batchnorm=args.use_batchnorm,\n",
    "            encodeLayer=[256, 64], decodeLayer=[64, 256], sigma=args.sigma, gamma=args.gamma, device=args.device)\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c272d6",
   "metadata": {},
   "source": [
    "Pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bbcfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> loading checkpoint '10xPBMC_with_attn_plus_layer_weights.pth.tar'\n",
      "Pretraining time: 0 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 196, ZINB loss: 0.22281503\n",
      "Pretrain epoch 197, ZINB loss: 0.22263191\n",
      "Pretrain epoch 198, ZINB loss: 0.22256174\n",
      "Pretrain epoch 199, ZINB loss: 0.22243141\n",
      "Pretrain epoch 200, ZINB loss: 0.22252310\n",
      "Pretrain epoch 201, ZINB loss: 0.22236248\n",
      "Pretrain epoch 202, ZINB loss: 0.22244692\n",
      "Pretrain epoch 203, ZINB loss: 0.22217898\n",
      "Pretrain epoch 204, ZINB loss: 0.22209356\n",
      "Pretrain epoch 205, ZINB loss: 0.22210407\n",
      "Pretrain epoch 206, ZINB loss: 0.22195778\n",
      "Pretrain epoch 207, ZINB loss: 0.22193105\n",
      "Pretrain epoch 208, ZINB loss: 0.22195967\n",
      "Pretrain epoch 209, ZINB loss: 0.22182915\n",
      "Pretrain epoch 210, ZINB loss: 0.22169341\n",
      "Pretrain epoch 211, ZINB loss: 0.22163370\n",
      "Pretrain epoch 212, ZINB loss: 0.22177499\n",
      "Pretrain epoch 213, ZINB loss: 0.22147643\n",
      "Pretrain epoch 214, ZINB loss: 0.22148880\n",
      "Pretrain epoch 215, ZINB loss: 0.22136244\n",
      "Pretrain epoch 216, ZINB loss: 0.22121936\n",
      "Pretrain epoch 217, ZINB loss: 0.22130410\n",
      "Pretrain epoch 218, ZINB loss: 0.22126952\n",
      "Pretrain epoch 219, ZINB loss: 0.22104641\n",
      "Pretrain epoch 220, ZINB loss: 0.22118292\n",
      "Pretrain epoch 221, ZINB loss: 0.22099563\n",
      "Pretrain epoch 222, ZINB loss: 0.22087587\n",
      "Pretrain epoch 223, ZINB loss: 0.22089438\n",
      "Pretrain epoch 224, ZINB loss: 0.22074216\n",
      "Pretrain epoch 225, ZINB loss: 0.22068737\n",
      "Pretrain epoch 226, ZINB loss: 0.22068257\n",
      "Pretrain epoch 227, ZINB loss: 0.22058894\n",
      "Pretrain epoch 228, ZINB loss: 0.22039792\n",
      "Pretrain epoch 229, ZINB loss: 0.22041764\n",
      "Pretrain epoch 230, ZINB loss: 0.22024141\n",
      "Pretrain epoch 231, ZINB loss: 0.22030703\n",
      "Pretrain epoch 232, ZINB loss: 0.22022413\n",
      "Pretrain epoch 233, ZINB loss: 0.22024772\n",
      "Pretrain epoch 234, ZINB loss: 0.22006695\n",
      "Pretrain epoch 235, ZINB loss: 0.21997589\n",
      "Pretrain epoch 236, ZINB loss: 0.22000271\n",
      "Pretrain epoch 237, ZINB loss: 0.21992193\n",
      "Pretrain epoch 238, ZINB loss: 0.21974672\n",
      "Pretrain epoch 239, ZINB loss: 0.21986109\n",
      "Pretrain epoch 240, ZINB loss: 0.21979446\n",
      "Pretrain epoch 241, ZINB loss: 0.21957877\n",
      "Pretrain epoch 242, ZINB loss: 0.21975860\n",
      "Pretrain epoch 243, ZINB loss: 0.21946437\n",
      "Pretrain epoch 244, ZINB loss: 0.21933061\n",
      "Pretrain epoch 245, ZINB loss: 0.21927085\n",
      "Pretrain epoch 246, ZINB loss: 0.21919537\n",
      "Pretrain epoch 247, ZINB loss: 0.21924410\n",
      "Pretrain epoch 248, ZINB loss: 0.21913700\n",
      "Pretrain epoch 249, ZINB loss: 0.21917329\n",
      "Pretrain epoch 250, ZINB loss: 0.21931821\n",
      "Pretrain epoch 251, ZINB loss: 0.21905606\n",
      "Pretrain epoch 252, ZINB loss: 0.21905883\n",
      "Pretrain epoch 253, ZINB loss: 0.21893270\n",
      "Pretrain epoch 254, ZINB loss: 0.21893617\n",
      "Pretrain epoch 255, ZINB loss: 0.21871363\n",
      "Pretrain epoch 256, ZINB loss: 0.21862733\n",
      "Pretrain epoch 257, ZINB loss: 0.21849339\n",
      "Pretrain epoch 258, ZINB loss: 0.21843390\n",
      "Pretrain epoch 259, ZINB loss: 0.21844238\n",
      "Pretrain epoch 260, ZINB loss: 0.21863204\n",
      "Pretrain epoch 261, ZINB loss: 0.21838971\n",
      "Pretrain epoch 262, ZINB loss: 0.21820849\n",
      "Pretrain epoch 263, ZINB loss: 0.21811830\n",
      "Pretrain epoch 264, ZINB loss: 0.21810944\n",
      "Pretrain epoch 265, ZINB loss: 0.21804571\n",
      "Pretrain epoch 266, ZINB loss: 0.21792821\n",
      "Pretrain epoch 267, ZINB loss: 0.21811300\n",
      "Pretrain epoch 268, ZINB loss: 0.21798248\n",
      "Pretrain epoch 269, ZINB loss: 0.21796048\n",
      "Pretrain epoch 270, ZINB loss: 0.21772245\n",
      "Pretrain epoch 271, ZINB loss: 0.21774004\n",
      "Pretrain epoch 272, ZINB loss: 0.21776114\n",
      "Pretrain epoch 273, ZINB loss: 0.21758871\n",
      "Pretrain epoch 274, ZINB loss: 0.21752864\n",
      "Pretrain epoch 275, ZINB loss: 0.21744584\n",
      "Pretrain epoch 276, ZINB loss: 0.21749883\n",
      "Pretrain epoch 277, ZINB loss: 0.21763060\n",
      "Pretrain epoch 278, ZINB loss: 0.21723779\n",
      "Pretrain epoch 279, ZINB loss: 0.21728777\n",
      "Pretrain epoch 280, ZINB loss: 0.21727359\n",
      "Pretrain epoch 281, ZINB loss: 0.21724647\n",
      "Pretrain epoch 282, ZINB loss: 0.21719001\n",
      "Pretrain epoch 283, ZINB loss: 0.21702635\n",
      "Pretrain epoch 284, ZINB loss: 0.21690302\n",
      "Pretrain epoch 285, ZINB loss: 0.21694075\n",
      "Pretrain epoch 286, ZINB loss: 0.21681773\n",
      "Pretrain epoch 287, ZINB loss: 0.21676968\n",
      "Pretrain epoch 288, ZINB loss: 0.21679875\n",
      "Pretrain epoch 289, ZINB loss: 0.21676224\n",
      "Pretrain epoch 290, ZINB loss: 0.21675163\n",
      "Pretrain epoch 291, ZINB loss: 0.21686860\n",
      "Pretrain epoch 292, ZINB loss: 0.21687557\n",
      "Pretrain epoch 293, ZINB loss: 0.21667668\n",
      "Pretrain epoch 294, ZINB loss: 0.21652196\n",
      "Pretrain epoch 295, ZINB loss: 0.21626951\n",
      "Pretrain epoch 296, ZINB loss: 0.21622735\n",
      "Pretrain epoch 297, ZINB loss: 0.21628966\n",
      "Pretrain epoch 298, ZINB loss: 0.21629726\n",
      "Pretrain epoch 299, ZINB loss: 0.21615532\n",
      "Pretrain epoch 300, ZINB loss: 0.21604352\n",
      "Pretraining time: 4928 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "if args.ae_weights is None:\n",
    "    model.pretrain_autoencoder(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, \n",
    "                            batch_size=args.batch_size, epochs=args.pretrain_epochs, ae_weights=args.ae_weight_file)\n",
    "else:\n",
    "    if os.path.isfile(args.ae_weights):\n",
    "        print(\"==> loading checkpoint '{}'\".format(args.ae_weights))\n",
    "        checkpoint = torch.load(args.ae_weights)\n",
    "        model.load_state_dict(checkpoint['ae_state_dict'])\n",
    "    else:\n",
    "        print(\"==> no checkpoint found at '{}'\".format(args.ae_weights))\n",
    "        raise ValueError\n",
    "\n",
    "print('Pretraining time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0ded9",
   "metadata": {},
   "source": [
    "Clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1a72f6-053d-4196-8922-f4dcacf9086b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.7310, ARI= 0.6351\n",
      "Clustering   1: NMI= 0.7310, ARI= 0.6351\n",
      "Epoch   1: Total: 0.36894200 Clustering Loss: 0.14473826 ZINB Loss: 0.22420374\n",
      "Clustering   2: NMI= 0.7229, ARI= 0.6196\n",
      "Epoch   2: Total: 0.39492024 Clustering Loss: 0.16923975 ZINB Loss: 0.22568049\n",
      "Clustering   3: NMI= 0.7249, ARI= 0.6139\n",
      "Epoch   3: Total: 0.41060636 Clustering Loss: 0.18289070 ZINB Loss: 0.22771567\n",
      "Clustering   4: NMI= 0.7307, ARI= 0.6224\n",
      "Epoch   4: Total: 0.41118640 Clustering Loss: 0.18189972 ZINB Loss: 0.22928668\n",
      "Clustering   5: NMI= 0.7262, ARI= 0.6114\n",
      "Epoch   5: Total: 0.39167214 Clustering Loss: 0.16158198 ZINB Loss: 0.23009016\n",
      "Clustering   6: NMI= 0.7331, ARI= 0.6182\n",
      "Epoch   6: Total: 0.39631587 Clustering Loss: 0.16541630 ZINB Loss: 0.23089957\n",
      "Clustering   7: NMI= 0.7269, ARI= 0.6094\n",
      "Epoch   7: Total: 0.37870506 Clustering Loss: 0.14741709 ZINB Loss: 0.23128798\n",
      "Clustering   8: NMI= 0.7287, ARI= 0.6066\n",
      "Epoch   8: Total: 0.38204418 Clustering Loss: 0.15040084 ZINB Loss: 0.23164335\n",
      "Clustering   9: NMI= 0.7284, ARI= 0.6037\n",
      "Epoch   9: Total: 0.37509022 Clustering Loss: 0.14333092 ZINB Loss: 0.23175929\n",
      "Clustering  10: NMI= 0.7277, ARI= 0.5995\n",
      "Epoch  10: Total: 0.37210879 Clustering Loss: 0.14003466 ZINB Loss: 0.23207413\n",
      "Clustering  11: NMI= 0.7278, ARI= 0.5988\n",
      "Epoch  11: Total: 0.36970357 Clustering Loss: 0.13762377 ZINB Loss: 0.23207980\n",
      "Clustering  12: NMI= 0.7273, ARI= 0.5973\n",
      "Epoch  12: Total: 0.36541728 Clustering Loss: 0.13348421 ZINB Loss: 0.23193307\n",
      "Clustering  13: NMI= 0.7272, ARI= 0.5964\n",
      "Epoch  13: Total: 0.36271184 Clustering Loss: 0.13067793 ZINB Loss: 0.23203391\n",
      "Clustering  14: NMI= 0.7267, ARI= 0.5956\n",
      "Epoch  14: Total: 0.35894342 Clustering Loss: 0.12696964 ZINB Loss: 0.23197378\n",
      "Clustering  15: NMI= 0.7259, ARI= 0.5943\n",
      "Epoch  15: Total: 0.35566292 Clustering Loss: 0.12381850 ZINB Loss: 0.23184442\n",
      "Clustering  16: NMI= 0.7256, ARI= 0.5939\n",
      "delta_label 0.000702 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 150 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5bbe80-5c51-42c4-b392-41c125ed1a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters:  12\n",
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.6696, ARI= 0.4666\n",
      "Clustering   1: NMI= 0.6923, ARI= 0.4913\n",
      "Epoch   1: Total: 0.40360253 Clustering Loss: 0.17945546 ZINB Loss: 0.22414707\n",
      "Clustering   2: NMI= 0.6945, ARI= 0.4920\n",
      "Epoch   2: Total: 0.45734345 Clustering Loss: 0.23194591 ZINB Loss: 0.22539754\n",
      "Clustering   3: NMI= 0.6935, ARI= 0.4890\n",
      "Epoch   3: Total: 0.48689302 Clustering Loss: 0.26006024 ZINB Loss: 0.22683278\n",
      "Clustering   4: NMI= 0.6929, ARI= 0.4876\n",
      "Epoch   4: Total: 0.49999684 Clustering Loss: 0.27199246 ZINB Loss: 0.22800438\n",
      "Clustering   5: NMI= 0.6911, ARI= 0.4860\n",
      "Epoch   5: Total: 0.47606372 Clustering Loss: 0.24733027 ZINB Loss: 0.22873345\n",
      "Clustering   6: NMI= 0.6931, ARI= 0.4853\n",
      "Epoch   6: Total: 0.48568799 Clustering Loss: 0.25619388 ZINB Loss: 0.22949412\n",
      "Clustering   7: NMI= 0.6914, ARI= 0.4873\n",
      "Epoch   7: Total: 0.46683215 Clustering Loss: 0.23700514 ZINB Loss: 0.22982701\n",
      "Clustering   8: NMI= 0.6941, ARI= 0.4841\n",
      "Epoch   8: Total: 0.44913570 Clustering Loss: 0.21895670 ZINB Loss: 0.23017900\n",
      "Clustering   9: NMI= 0.6932, ARI= 0.4872\n",
      "Epoch   9: Total: 0.45261554 Clustering Loss: 0.22228806 ZINB Loss: 0.23032747\n",
      "Clustering  10: NMI= 0.6928, ARI= 0.4848\n",
      "Epoch  10: Total: 0.44653363 Clustering Loss: 0.21581932 ZINB Loss: 0.23071432\n",
      "Clustering  11: NMI= 0.6912, ARI= 0.4843\n",
      "Epoch  11: Total: 0.43298167 Clustering Loss: 0.20204185 ZINB Loss: 0.23093982\n",
      "Clustering  12: NMI= 0.6910, ARI= 0.4840\n",
      "Epoch  12: Total: 0.44531715 Clustering Loss: 0.21421936 ZINB Loss: 0.23109779\n",
      "Clustering  13: NMI= 0.6906, ARI= 0.4834\n",
      "Epoch  13: Total: 0.42257310 Clustering Loss: 0.19144612 ZINB Loss: 0.23112698\n",
      "Clustering  14: NMI= 0.6903, ARI= 0.4825\n",
      "Epoch  14: Total: 0.43401551 Clustering Loss: 0.20268768 ZINB Loss: 0.23132782\n",
      "Clustering  15: NMI= 0.6908, ARI= 0.4829\n",
      "Epoch  15: Total: 0.41926611 Clustering Loss: 0.18797149 ZINB Loss: 0.23129463\n",
      "Clustering  16: NMI= 0.6912, ARI= 0.4825\n",
      "Epoch  16: Total: 0.42377843 Clustering Loss: 0.19223284 ZINB Loss: 0.23154558\n",
      "Clustering  17: NMI= 0.6911, ARI= 0.4827\n",
      "Epoch  17: Total: 0.41520204 Clustering Loss: 0.18387768 ZINB Loss: 0.23132437\n",
      "Clustering  18: NMI= 0.6912, ARI= 0.4820\n",
      "Epoch  18: Total: 0.41351909 Clustering Loss: 0.18190916 ZINB Loss: 0.23160993\n",
      "Clustering  19: NMI= 0.6914, ARI= 0.4826\n",
      "Epoch  19: Total: 0.40908113 Clustering Loss: 0.17766967 ZINB Loss: 0.23141146\n",
      "Clustering  20: NMI= 0.6900, ARI= 0.4808\n",
      "Epoch  20: Total: 0.40657879 Clustering Loss: 0.17485855 ZINB Loss: 0.23172024\n",
      "Clustering  21: NMI= 0.6908, ARI= 0.4825\n",
      "Epoch  21: Total: 0.40547499 Clustering Loss: 0.17378556 ZINB Loss: 0.23168944\n",
      "Clustering  22: NMI= 0.6903, ARI= 0.4815\n",
      "Epoch  22: Total: 0.40168739 Clustering Loss: 0.17000217 ZINB Loss: 0.23168522\n",
      "Clustering  23: NMI= 0.6905, ARI= 0.4822\n",
      "Epoch  23: Total: 0.39743089 Clustering Loss: 0.16578358 ZINB Loss: 0.23164731\n",
      "Clustering  24: NMI= 0.6908, ARI= 0.4823\n",
      "Epoch  24: Total: 0.39552627 Clustering Loss: 0.16400182 ZINB Loss: 0.23152446\n",
      "Clustering  25: NMI= 0.6910, ARI= 0.4826\n",
      "Epoch  25: Total: 0.39077597 Clustering Loss: 0.15924004 ZINB Loss: 0.23153593\n",
      "Clustering  26: NMI= 0.6909, ARI= 0.4827\n",
      "Epoch  26: Total: 0.38952556 Clustering Loss: 0.15808877 ZINB Loss: 0.23143679\n",
      "Clustering  27: NMI= 0.6910, ARI= 0.4827\n",
      "Epoch  27: Total: 0.38760063 Clustering Loss: 0.15606586 ZINB Loss: 0.23153477\n",
      "Clustering  28: NMI= 0.6911, ARI= 0.4827\n",
      "Epoch  28: Total: 0.38601200 Clustering Loss: 0.15418331 ZINB Loss: 0.23182869\n",
      "Clustering  29: NMI= 0.6909, ARI= 0.4828\n",
      "Epoch  29: Total: 0.38314256 Clustering Loss: 0.15157070 ZINB Loss: 0.23157185\n",
      "Clustering  30: NMI= 0.6908, ARI= 0.4826\n",
      "Epoch  30: Total: 0.37779256 Clustering Loss: 0.14621402 ZINB Loss: 0.23157854\n",
      "Clustering  31: NMI= 0.6902, ARI= 0.4826\n",
      "Epoch  31: Total: 0.37794003 Clustering Loss: 0.14636143 ZINB Loss: 0.23157861\n",
      "Clustering  32: NMI= 0.6907, ARI= 0.4830\n",
      "Epoch  32: Total: 0.37331751 Clustering Loss: 0.14206881 ZINB Loss: 0.23124870\n",
      "Clustering  33: NMI= 0.6900, ARI= 0.4828\n",
      "Epoch  33: Total: 0.37169790 Clustering Loss: 0.14026968 ZINB Loss: 0.23142822\n",
      "Clustering  34: NMI= 0.6899, ARI= 0.4826\n",
      "delta_label 0.000702 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 329 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "args.n_clusters = 0\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548b5d",
   "metadata": {},
   "source": [
    "Output and save predicted labels and latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ae42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cells:\n",
      "  NMI = 0.7256\n",
      "  ARI = 0.5939\n",
      "  Silhouette Score = -0.0249\n",
      "  Calinski-Harabasz Index = 14.36\n"
     ]
    }
   ],
   "source": [
    "if y is not None:\n",
    "    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "    ss = np.round(metrics.silhouette_score(adata.X, y_pred), 5)\n",
    "    ch = np.round(metrics.calinski_harabasz_score(adata.X, y_pred), 2)\n",
    "    print('Evaluating cells:')\n",
    "    print(f'  NMI = {nmi:.4f}')\n",
    "    print(f'  ARI = {ari:.4f}')\n",
    "    print(f'  Silhouette Score = {ss:.4f}')\n",
    "    print(f'  Calinski-Harabasz Index = {ch:.2f}')\n",
    "\n",
    "final_latent = model.encodeBatch(torch.tensor(adata.X)).cpu().numpy()\n",
    "np.savetxt(args.final_latent_file, final_latent, delimiter=\",\")\n",
    "np.savetxt(args.predict_label_file, y_pred, delimiter=\",\", fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b8c19",
   "metadata": {},
   "source": [
    "Run t-SNE on latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60652b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "                    perplexity=30,\n",
    "                    initialization=\"pca\",\n",
    "                    metric=\"euclidean\",\n",
    "                    n_jobs=8,\n",
    "                    random_state=42,\n",
    "                )\n",
    "latent_tsne_2 = tsne_embedding.fit(final_latent)\n",
    "np.savetxt(\"tsne_2D.txt\", latent_tsne_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa113e8b",
   "metadata": {},
   "source": [
    "Plot 2D t-SNE of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls())\n",
    "library(ggplot2)\n",
    "\n",
    "latent_tsne <- read.table(\"tsne_2D.txt\", sep=\",\")\n",
    "colnames(latent_tsne) <- c(\"TSNE_1\", \"TSNE_2\")\n",
    "y_pred <- as.numeric(readLines(\"pred_labels.txt\"))\n",
    "y_pred <- factor(y_pred, levels=0:max(y_pred))\n",
    "\n",
    "dat <- data.frame(latent_tsne, y_pred=y_pred)\n",
    "\n",
    "ggplot(dat, aes(x=TSNE_1, y=TSNE_2, color=y_pred)) + geom_point() + theme_classic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4ae2-5110-41b5-84f1-866a5e532364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
