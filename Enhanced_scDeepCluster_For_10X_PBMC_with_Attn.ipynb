{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e53af6",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38911249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math, os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('./Baseline_Code')\n",
    "sys.path.append('./EnhancedscDeepCluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3b303c-8f5d-4703-8297-489cd88226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scDeepCluster\n",
    "from scDeepCluster import scDeepCluster\n",
    "from single_cell_tools import *\n",
    "from preprocess import read_dataset, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a532a6-d597-4cef-bb06-68648c1a72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModifiedscDeepCluster\n",
    "import model\n",
    "import model_single_cell_tools\n",
    "import model_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60af52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b52e630030>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for repeatability\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc9d56",
   "metadata": {},
   "source": [
    "Setup parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e62117",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter setting\n",
    "'''\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.n_clusters = 8\n",
    "        self.knn = 20\n",
    "        self.resolution = 0.8\n",
    "        self.select_genes = 0\n",
    "        self.batch_size = 256\n",
    "        self.data_file = '10X_PBMC.h5'\n",
    "        self.maxiter = 2000\n",
    "        self.pretrain_epochs = 300\n",
    "        self.gamma = 1.\n",
    "        self.sigma = 2.5\n",
    "        self.update_interval = 1\n",
    "        self.tol = 0.001\n",
    "        self.use_attention = True\n",
    "        self.use_layernorm = False\n",
    "        self.use_batchnorm = False\n",
    "        # self.ae_weights = None\n",
    "        self.ae_weights = '10xPBMC_with_attn_weights.pth.tar'\n",
    "        self.save_dir = 'results/EnhancedscDeepCluster/'\n",
    "        self.ae_weight_file = '10xPBMC_with_attn_weights.pth.tar'\n",
    "        self.final_latent_file = '10xPBMC_final_latent_file.txt'\n",
    "        self.predict_label_file = '10xPBMC_pred_labels.txt'\n",
    "        self.device = 'cuda'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590d2b",
   "metadata": {},
   "source": [
    "Normalizating and preprocessing count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1ae96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 16653 genes and 4271 cells.\n",
      "<__main__.Args object at 0x000001B534E1FA30>\n",
      "(4271, 16653)\n",
      "(4271,)\n"
     ]
    }
   ],
   "source": [
    "data_mat = h5py.File(args.data_file)\n",
    "x = np.array(data_mat['X'])\n",
    "# y is the ground truth labels for evaluating clustering performance\n",
    "# If not existing, we skip calculating the clustering performance metrics (e.g. NMI ARI)\n",
    "if 'Y' in data_mat:\n",
    "    y = np.array(data_mat['Y'])\n",
    "else:\n",
    "    y = None\n",
    "data_mat.close()\n",
    "\n",
    "if args.select_genes > 0:\n",
    "    importantGenes = model_single_cell_tools.geneSelection(x, n=args.select_genes, plot=False)\n",
    "    x = x[:, importantGenes]\n",
    "\n",
    "# preprocessing scRNA-seq read counts matrix\n",
    "adata = sc.AnnData(x)\n",
    "if y is not None:\n",
    "    adata.obs['Group'] = y\n",
    "\n",
    "adata = model_preprocess.read_dataset(adata,\n",
    "                 transpose=False,\n",
    "                 test_split=False,\n",
    "                 copy=True)\n",
    "\n",
    "adata = model_preprocess.normalize(adata,\n",
    "                  size_factors=True,\n",
    "                  normalize_input=True,\n",
    "                  logtrans_input=True)\n",
    "\n",
    "input_size = adata.n_vars\n",
    "\n",
    "print(args)\n",
    "\n",
    "print(adata.X.shape)\n",
    "if y is not None:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827573d",
   "metadata": {},
   "source": [
    "Build scDeepCluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d2f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedscDeepCluster(\n",
      "  (dropoutLayer): Dropout(p=0.8, inplace=False)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=16653, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (attention): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.EnhancedscDeepCluster(input_dim=adata.n_vars, z_dim=32, use_attention=args.use_attention, use_layernorm=args.use_layernorm, use_batchnorm=args.use_batchnorm,\n",
    "            encodeLayer=[256, 64], decodeLayer=[64, 256], sigma=args.sigma, gamma=args.gamma, device=args.device)\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c272d6",
   "metadata": {},
   "source": [
    "Pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bbcfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> loading checkpoint '10xPBMC_with_attn_weights.pth.tar'\n",
      "Pretraining time: 0 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 196, ZINB loss: 0.22281503\n",
      "Pretrain epoch 197, ZINB loss: 0.22263191\n",
      "Pretrain epoch 198, ZINB loss: 0.22256174\n",
      "Pretrain epoch 199, ZINB loss: 0.22243141\n",
      "Pretrain epoch 200, ZINB loss: 0.22252310\n",
      "Pretrain epoch 201, ZINB loss: 0.22236248\n",
      "Pretrain epoch 202, ZINB loss: 0.22244692\n",
      "Pretrain epoch 203, ZINB loss: 0.22217898\n",
      "Pretrain epoch 204, ZINB loss: 0.22209356\n",
      "Pretrain epoch 205, ZINB loss: 0.22210407\n",
      "Pretrain epoch 206, ZINB loss: 0.22195778\n",
      "Pretrain epoch 207, ZINB loss: 0.22193105\n",
      "Pretrain epoch 208, ZINB loss: 0.22195967\n",
      "Pretrain epoch 209, ZINB loss: 0.22182915\n",
      "Pretrain epoch 210, ZINB loss: 0.22169341\n",
      "Pretrain epoch 211, ZINB loss: 0.22163370\n",
      "Pretrain epoch 212, ZINB loss: 0.22177499\n",
      "Pretrain epoch 213, ZINB loss: 0.22147643\n",
      "Pretrain epoch 214, ZINB loss: 0.22148880\n",
      "Pretrain epoch 215, ZINB loss: 0.22136244\n",
      "Pretrain epoch 216, ZINB loss: 0.22121936\n",
      "Pretrain epoch 217, ZINB loss: 0.22130410\n",
      "Pretrain epoch 218, ZINB loss: 0.22126952\n",
      "Pretrain epoch 219, ZINB loss: 0.22104641\n",
      "Pretrain epoch 220, ZINB loss: 0.22118292\n",
      "Pretrain epoch 221, ZINB loss: 0.22099563\n",
      "Pretrain epoch 222, ZINB loss: 0.22087587\n",
      "Pretrain epoch 223, ZINB loss: 0.22089438\n",
      "Pretrain epoch 224, ZINB loss: 0.22074216\n",
      "Pretrain epoch 225, ZINB loss: 0.22068737\n",
      "Pretrain epoch 226, ZINB loss: 0.22068257\n",
      "Pretrain epoch 227, ZINB loss: 0.22058894\n",
      "Pretrain epoch 228, ZINB loss: 0.22039792\n",
      "Pretrain epoch 229, ZINB loss: 0.22041764\n",
      "Pretrain epoch 230, ZINB loss: 0.22024141\n",
      "Pretrain epoch 231, ZINB loss: 0.22030703\n",
      "Pretrain epoch 232, ZINB loss: 0.22022413\n",
      "Pretrain epoch 233, ZINB loss: 0.22024772\n",
      "Pretrain epoch 234, ZINB loss: 0.22006695\n",
      "Pretrain epoch 235, ZINB loss: 0.21997589\n",
      "Pretrain epoch 236, ZINB loss: 0.22000271\n",
      "Pretrain epoch 237, ZINB loss: 0.21992193\n",
      "Pretrain epoch 238, ZINB loss: 0.21974672\n",
      "Pretrain epoch 239, ZINB loss: 0.21986109\n",
      "Pretrain epoch 240, ZINB loss: 0.21979446\n",
      "Pretrain epoch 241, ZINB loss: 0.21957877\n",
      "Pretrain epoch 242, ZINB loss: 0.21975860\n",
      "Pretrain epoch 243, ZINB loss: 0.21946437\n",
      "Pretrain epoch 244, ZINB loss: 0.21933061\n",
      "Pretrain epoch 245, ZINB loss: 0.21927085\n",
      "Pretrain epoch 246, ZINB loss: 0.21919537\n",
      "Pretrain epoch 247, ZINB loss: 0.21924410\n",
      "Pretrain epoch 248, ZINB loss: 0.21913700\n",
      "Pretrain epoch 249, ZINB loss: 0.21917329\n",
      "Pretrain epoch 250, ZINB loss: 0.21931821\n",
      "Pretrain epoch 251, ZINB loss: 0.21905606\n",
      "Pretrain epoch 252, ZINB loss: 0.21905883\n",
      "Pretrain epoch 253, ZINB loss: 0.21893270\n",
      "Pretrain epoch 254, ZINB loss: 0.21893617\n",
      "Pretrain epoch 255, ZINB loss: 0.21871363\n",
      "Pretrain epoch 256, ZINB loss: 0.21862733\n",
      "Pretrain epoch 257, ZINB loss: 0.21849339\n",
      "Pretrain epoch 258, ZINB loss: 0.21843390\n",
      "Pretrain epoch 259, ZINB loss: 0.21844238\n",
      "Pretrain epoch 260, ZINB loss: 0.21863204\n",
      "Pretrain epoch 261, ZINB loss: 0.21838971\n",
      "Pretrain epoch 262, ZINB loss: 0.21820849\n",
      "Pretrain epoch 263, ZINB loss: 0.21811830\n",
      "Pretrain epoch 264, ZINB loss: 0.21810944\n",
      "Pretrain epoch 265, ZINB loss: 0.21804571\n",
      "Pretrain epoch 266, ZINB loss: 0.21792821\n",
      "Pretrain epoch 267, ZINB loss: 0.21811300\n",
      "Pretrain epoch 268, ZINB loss: 0.21798248\n",
      "Pretrain epoch 269, ZINB loss: 0.21796048\n",
      "Pretrain epoch 270, ZINB loss: 0.21772245\n",
      "Pretrain epoch 271, ZINB loss: 0.21774004\n",
      "Pretrain epoch 272, ZINB loss: 0.21776114\n",
      "Pretrain epoch 273, ZINB loss: 0.21758871\n",
      "Pretrain epoch 274, ZINB loss: 0.21752864\n",
      "Pretrain epoch 275, ZINB loss: 0.21744584\n",
      "Pretrain epoch 276, ZINB loss: 0.21749883\n",
      "Pretrain epoch 277, ZINB loss: 0.21763060\n",
      "Pretrain epoch 278, ZINB loss: 0.21723779\n",
      "Pretrain epoch 279, ZINB loss: 0.21728777\n",
      "Pretrain epoch 280, ZINB loss: 0.21727359\n",
      "Pretrain epoch 281, ZINB loss: 0.21724647\n",
      "Pretrain epoch 282, ZINB loss: 0.21719001\n",
      "Pretrain epoch 283, ZINB loss: 0.21702635\n",
      "Pretrain epoch 284, ZINB loss: 0.21690302\n",
      "Pretrain epoch 285, ZINB loss: 0.21694075\n",
      "Pretrain epoch 286, ZINB loss: 0.21681773\n",
      "Pretrain epoch 287, ZINB loss: 0.21676968\n",
      "Pretrain epoch 288, ZINB loss: 0.21679875\n",
      "Pretrain epoch 289, ZINB loss: 0.21676224\n",
      "Pretrain epoch 290, ZINB loss: 0.21675163\n",
      "Pretrain epoch 291, ZINB loss: 0.21686860\n",
      "Pretrain epoch 292, ZINB loss: 0.21687557\n",
      "Pretrain epoch 293, ZINB loss: 0.21667668\n",
      "Pretrain epoch 294, ZINB loss: 0.21652196\n",
      "Pretrain epoch 295, ZINB loss: 0.21626951\n",
      "Pretrain epoch 296, ZINB loss: 0.21622735\n",
      "Pretrain epoch 297, ZINB loss: 0.21628966\n",
      "Pretrain epoch 298, ZINB loss: 0.21629726\n",
      "Pretrain epoch 299, ZINB loss: 0.21615532\n",
      "Pretrain epoch 300, ZINB loss: 0.21604352\n",
      "Pretraining time: 4928 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "if args.ae_weights is None:\n",
    "    model.pretrain_autoencoder(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, \n",
    "                            batch_size=args.batch_size, epochs=args.pretrain_epochs, ae_weights=args.ae_weight_file)\n",
    "else:\n",
    "    if os.path.isfile(args.ae_weights):\n",
    "        print(\"==> loading checkpoint '{}'\".format(args.ae_weights))\n",
    "        checkpoint = torch.load(args.ae_weights)\n",
    "        model.load_state_dict(checkpoint['ae_state_dict'])\n",
    "    else:\n",
    "        print(\"==> no checkpoint found at '{}'\".format(args.ae_weights))\n",
    "        raise ValueError\n",
    "\n",
    "print('Pretraining time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0ded9",
   "metadata": {},
   "source": [
    "Clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1a72f6-053d-4196-8922-f4dcacf9086b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.7032, ARI= 0.6330\n",
      "Clustering   1: NMI= 0.7032, ARI= 0.6330\n",
      "Epoch   1: Total: 0.41969351 Clustering Loss: 0.18665691 ZINB Loss: 0.23303660\n",
      "Clustering   2: NMI= 0.7058, ARI= 0.6246\n",
      "Epoch   2: Total: 0.48050930 Clustering Loss: 0.24484997 ZINB Loss: 0.23565933\n",
      "Clustering   3: NMI= 0.7212, ARI= 0.6369\n",
      "Epoch   3: Total: 0.50090901 Clustering Loss: 0.26486091 ZINB Loss: 0.23604809\n",
      "Clustering   4: NMI= 0.7177, ARI= 0.6189\n",
      "Epoch   4: Total: 0.51091165 Clustering Loss: 0.27372247 ZINB Loss: 0.23718918\n",
      "Clustering   5: NMI= 0.7282, ARI= 0.6232\n",
      "Epoch   5: Total: 0.48626468 Clustering Loss: 0.24960260 ZINB Loss: 0.23666209\n",
      "Clustering   6: NMI= 0.7184, ARI= 0.6155\n",
      "Epoch   6: Total: 0.51626250 Clustering Loss: 0.27875445 ZINB Loss: 0.23750804\n",
      "Clustering   7: NMI= 0.7288, ARI= 0.6230\n",
      "Epoch   7: Total: 0.46770886 Clustering Loss: 0.23032190 ZINB Loss: 0.23738696\n",
      "Clustering   8: NMI= 0.7235, ARI= 0.6191\n",
      "Epoch   8: Total: 0.52909985 Clustering Loss: 0.29113359 ZINB Loss: 0.23796626\n",
      "Clustering   9: NMI= 0.7234, ARI= 0.6169\n",
      "Epoch   9: Total: 0.45863648 Clustering Loss: 0.22112583 ZINB Loss: 0.23751065\n",
      "Clustering  10: NMI= 0.7220, ARI= 0.6169\n",
      "Epoch  10: Total: 0.52613915 Clustering Loss: 0.28827950 ZINB Loss: 0.23785965\n",
      "Clustering  11: NMI= 0.7256, ARI= 0.6169\n",
      "Epoch  11: Total: 0.46631061 Clustering Loss: 0.22808323 ZINB Loss: 0.23822739\n",
      "Clustering  12: NMI= 0.7246, ARI= 0.6184\n",
      "Epoch  12: Total: 0.51587801 Clustering Loss: 0.27830535 ZINB Loss: 0.23757266\n",
      "Clustering  13: NMI= 0.7280, ARI= 0.6176\n",
      "Epoch  13: Total: 0.47657139 Clustering Loss: 0.23845051 ZINB Loss: 0.23812087\n",
      "Clustering  14: NMI= 0.7252, ARI= 0.6174\n",
      "Epoch  14: Total: 0.50616782 Clustering Loss: 0.26880499 ZINB Loss: 0.23736283\n",
      "Clustering  15: NMI= 0.7282, ARI= 0.6176\n",
      "Epoch  15: Total: 0.48244771 Clustering Loss: 0.24409660 ZINB Loss: 0.23835111\n",
      "Clustering  16: NMI= 0.7256, ARI= 0.6173\n",
      "Epoch  16: Total: 0.49574636 Clustering Loss: 0.25821428 ZINB Loss: 0.23753207\n",
      "Clustering  17: NMI= 0.7277, ARI= 0.6172\n",
      "Epoch  17: Total: 0.48512102 Clustering Loss: 0.24741012 ZINB Loss: 0.23771090\n",
      "Clustering  18: NMI= 0.7268, ARI= 0.6173\n",
      "Epoch  18: Total: 0.48752621 Clustering Loss: 0.24995157 ZINB Loss: 0.23757465\n",
      "Clustering  19: NMI= 0.7290, ARI= 0.6183\n",
      "Epoch  19: Total: 0.48590912 Clustering Loss: 0.24861257 ZINB Loss: 0.23729655\n",
      "Clustering  20: NMI= 0.7258, ARI= 0.6158\n",
      "Epoch  20: Total: 0.48191146 Clustering Loss: 0.24430021 ZINB Loss: 0.23761124\n",
      "Clustering  21: NMI= 0.7294, ARI= 0.6182\n",
      "Epoch  21: Total: 0.48651441 Clustering Loss: 0.24944205 ZINB Loss: 0.23707236\n",
      "Clustering  22: NMI= 0.7268, ARI= 0.6164\n",
      "Epoch  22: Total: 0.47489249 Clustering Loss: 0.23802842 ZINB Loss: 0.23686407\n",
      "Clustering  23: NMI= 0.7292, ARI= 0.6182\n",
      "Epoch  23: Total: 0.48588783 Clustering Loss: 0.24795287 ZINB Loss: 0.23793496\n",
      "Clustering  24: NMI= 0.7271, ARI= 0.6165\n",
      "Epoch  24: Total: 0.46963955 Clustering Loss: 0.23202522 ZINB Loss: 0.23761433\n",
      "Clustering  25: NMI= 0.7282, ARI= 0.6175\n",
      "Epoch  25: Total: 0.49188333 Clustering Loss: 0.25453977 ZINB Loss: 0.23734356\n",
      "Clustering  26: NMI= 0.7264, ARI= 0.6163\n",
      "Epoch  26: Total: 0.46114257 Clustering Loss: 0.22482062 ZINB Loss: 0.23632194\n",
      "Clustering  27: NMI= 0.7287, ARI= 0.6179\n",
      "Epoch  27: Total: 0.47958719 Clustering Loss: 0.24180463 ZINB Loss: 0.23778256\n",
      "Clustering  28: NMI= 0.7267, ARI= 0.6166\n",
      "Epoch  28: Total: 0.45940701 Clustering Loss: 0.22205802 ZINB Loss: 0.23734899\n",
      "Clustering  29: NMI= 0.7278, ARI= 0.6171\n",
      "Epoch  29: Total: 0.47856364 Clustering Loss: 0.24127173 ZINB Loss: 0.23729191\n",
      "Clustering  30: NMI= 0.7269, ARI= 0.6165\n",
      "Epoch  30: Total: 0.45389621 Clustering Loss: 0.21760135 ZINB Loss: 0.23629486\n",
      "Clustering  31: NMI= 0.7279, ARI= 0.6173\n",
      "delta_label 0.000937 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 226 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf65203-d15c-4dc3-9d93-540aa51278d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters:  13\n",
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.6908, ARI= 0.4810\n",
      "Clustering   1: NMI= 0.6925, ARI= 0.4766\n",
      "Epoch   1: Total: 0.50454393 Clustering Loss: 0.27135825 ZINB Loss: 0.23318568\n",
      "Clustering   2: NMI= 0.6869, ARI= 0.4965\n",
      "Epoch   2: Total: 0.54942749 Clustering Loss: 0.31568833 ZINB Loss: 0.23373917\n",
      "Clustering   3: NMI= 0.6932, ARI= 0.4811\n",
      "Epoch   3: Total: 0.53770469 Clustering Loss: 0.30369097 ZINB Loss: 0.23401372\n",
      "Clustering   4: NMI= 0.6896, ARI= 0.4803\n",
      "Epoch   4: Total: 0.54252399 Clustering Loss: 0.30864393 ZINB Loss: 0.23388006\n",
      "Clustering   5: NMI= 0.6969, ARI= 0.4835\n",
      "Epoch   5: Total: 0.57899771 Clustering Loss: 0.34368819 ZINB Loss: 0.23530952\n",
      "Clustering   6: NMI= 0.6921, ARI= 0.4786\n",
      "Epoch   6: Total: 0.59556295 Clustering Loss: 0.36054481 ZINB Loss: 0.23501814\n",
      "Clustering   7: NMI= 0.6964, ARI= 0.4789\n",
      "Epoch   7: Total: 0.61556164 Clustering Loss: 0.37970253 ZINB Loss: 0.23585911\n",
      "Clustering   8: NMI= 0.6915, ARI= 0.4768\n",
      "Epoch   8: Total: 0.63832228 Clustering Loss: 0.40313258 ZINB Loss: 0.23518970\n",
      "Clustering   9: NMI= 0.6977, ARI= 0.4791\n",
      "Epoch   9: Total: 0.53970176 Clustering Loss: 0.30397860 ZINB Loss: 0.23572316\n",
      "Clustering  10: NMI= 0.6935, ARI= 0.4761\n",
      "Epoch  10: Total: 0.66966690 Clustering Loss: 0.43251798 ZINB Loss: 0.23714892\n",
      "Clustering  11: NMI= 0.6959, ARI= 0.4762\n",
      "Epoch  11: Total: 0.55286721 Clustering Loss: 0.31561581 ZINB Loss: 0.23725141\n",
      "Clustering  12: NMI= 0.6944, ARI= 0.4735\n",
      "Epoch  12: Total: 0.66126814 Clustering Loss: 0.42448564 ZINB Loss: 0.23678251\n",
      "Clustering  13: NMI= 0.6905, ARI= 0.4745\n",
      "Epoch  13: Total: 0.58193656 Clustering Loss: 0.34465826 ZINB Loss: 0.23727830\n",
      "Clustering  14: NMI= 0.6953, ARI= 0.4780\n",
      "Epoch  14: Total: 0.69003006 Clustering Loss: 0.45331903 ZINB Loss: 0.23671102\n",
      "Clustering  15: NMI= 0.6927, ARI= 0.4745\n",
      "Epoch  15: Total: 0.63687801 Clustering Loss: 0.40060521 ZINB Loss: 0.23627280\n",
      "Clustering  16: NMI= 0.6932, ARI= 0.4747\n",
      "Epoch  16: Total: 0.59450883 Clustering Loss: 0.35783074 ZINB Loss: 0.23667809\n",
      "Clustering  17: NMI= 0.6918, ARI= 0.4739\n",
      "Epoch  17: Total: 0.61856590 Clustering Loss: 0.38195552 ZINB Loss: 0.23661039\n",
      "Clustering  18: NMI= 0.6929, ARI= 0.4740\n",
      "Epoch  18: Total: 0.62180832 Clustering Loss: 0.38539303 ZINB Loss: 0.23641529\n",
      "Clustering  19: NMI= 0.6907, ARI= 0.4725\n",
      "Epoch  19: Total: 0.64233183 Clustering Loss: 0.40519263 ZINB Loss: 0.23713920\n",
      "Clustering  20: NMI= 0.6902, ARI= 0.4715\n",
      "Epoch  20: Total: 0.74764878 Clustering Loss: 0.51123589 ZINB Loss: 0.23641289\n",
      "Clustering  21: NMI= 0.6925, ARI= 0.4731\n",
      "Epoch  21: Total: 0.54266220 Clustering Loss: 0.30588767 ZINB Loss: 0.23677453\n",
      "Clustering  22: NMI= 0.6918, ARI= 0.4731\n",
      "Epoch  22: Total: 0.61556501 Clustering Loss: 0.37872458 ZINB Loss: 0.23684043\n",
      "Clustering  23: NMI= 0.6893, ARI= 0.4711\n",
      "Epoch  23: Total: 0.61081952 Clustering Loss: 0.37374195 ZINB Loss: 0.23707757\n",
      "Clustering  24: NMI= 0.6913, ARI= 0.4722\n",
      "Epoch  24: Total: 0.62012954 Clustering Loss: 0.38387659 ZINB Loss: 0.23625295\n",
      "Clustering  25: NMI= 0.6895, ARI= 0.4717\n",
      "Epoch  25: Total: 0.57328928 Clustering Loss: 0.33705638 ZINB Loss: 0.23623289\n",
      "Clustering  26: NMI= 0.6899, ARI= 0.4719\n",
      "Epoch  26: Total: 0.65719413 Clustering Loss: 0.42071141 ZINB Loss: 0.23648272\n",
      "Clustering  27: NMI= 0.6904, ARI= 0.4716\n",
      "Epoch  27: Total: 0.57521348 Clustering Loss: 0.33949645 ZINB Loss: 0.23571703\n",
      "Clustering  28: NMI= 0.6887, ARI= 0.4713\n",
      "Epoch  28: Total: 0.61019427 Clustering Loss: 0.37328647 ZINB Loss: 0.23690780\n",
      "Clustering  29: NMI= 0.6899, ARI= 0.4716\n",
      "Epoch  29: Total: 0.57751502 Clustering Loss: 0.34107090 ZINB Loss: 0.23644412\n",
      "Clustering  30: NMI= 0.6891, ARI= 0.4715\n",
      "Epoch  30: Total: 0.59548920 Clustering Loss: 0.35891900 ZINB Loss: 0.23657020\n",
      "Clustering  31: NMI= 0.6904, ARI= 0.4720\n",
      "Epoch  31: Total: 0.47435555 Clustering Loss: 0.23848477 ZINB Loss: 0.23587078\n",
      "Clustering  32: NMI= 0.6893, ARI= 0.4715\n",
      "Epoch  32: Total: 0.56174543 Clustering Loss: 0.32526378 ZINB Loss: 0.23648165\n",
      "Clustering  33: NMI= 0.6896, ARI= 0.4715\n",
      "delta_label 0.000468 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 291 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "args.n_clusters = 0\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548b5d",
   "metadata": {},
   "source": [
    "Output and save predicted labels and latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ae42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cells:\n",
      "  NMI = 0.7279\n",
      "  ARI = 0.6173\n",
      "  Silhouette Score = -0.0244\n",
      "  Calinski-Harabasz Index = 14.28\n"
     ]
    }
   ],
   "source": [
    "if y is not None:\n",
    "    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "    ss = np.round(metrics.silhouette_score(adata.X, y_pred), 5)\n",
    "    ch = np.round(metrics.calinski_harabasz_score(adata.X, y_pred), 2)\n",
    "    print('Evaluating cells:')\n",
    "    print(f'  NMI = {nmi:.4f}')\n",
    "    print(f'  ARI = {ari:.4f}')\n",
    "    print(f'  Silhouette Score = {ss:.4f}')\n",
    "    print(f'  Calinski-Harabasz Index = {ch:.2f}')\n",
    "\n",
    "final_latent = model.encodeBatch(torch.tensor(adata.X)).cpu().numpy()\n",
    "np.savetxt(args.final_latent_file, final_latent, delimiter=\",\")\n",
    "np.savetxt(args.predict_label_file, y_pred, delimiter=\",\", fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b8c19",
   "metadata": {},
   "source": [
    "Run t-SNE on latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60652b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "                    perplexity=30,\n",
    "                    initialization=\"pca\",\n",
    "                    metric=\"euclidean\",\n",
    "                    n_jobs=8,\n",
    "                    random_state=42,\n",
    "                )\n",
    "latent_tsne_2 = tsne_embedding.fit(final_latent)\n",
    "np.savetxt(\"tsne_2D.txt\", latent_tsne_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa113e8b",
   "metadata": {},
   "source": [
    "Plot 2D t-SNE of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls())\n",
    "library(ggplot2)\n",
    "\n",
    "latent_tsne <- read.table(\"tsne_2D.txt\", sep=\",\")\n",
    "colnames(latent_tsne) <- c(\"TSNE_1\", \"TSNE_2\")\n",
    "y_pred <- as.numeric(readLines(\"pred_labels.txt\"))\n",
    "y_pred <- factor(y_pred, levels=0:max(y_pred))\n",
    "\n",
    "dat <- data.frame(latent_tsne, y_pred=y_pred)\n",
    "\n",
    "ggplot(dat, aes(x=TSNE_1, y=TSNE_2, color=y_pred)) + geom_point() + theme_classic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4ae2-5110-41b5-84f1-866a5e532364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
