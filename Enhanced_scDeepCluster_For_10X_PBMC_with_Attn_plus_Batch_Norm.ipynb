{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e53af6",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38911249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math, os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('./Baseline_Code')\n",
    "sys.path.append('./EnhancedscDeepCluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3b303c-8f5d-4703-8297-489cd88226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scDeepCluster\n",
    "from scDeepCluster import scDeepCluster\n",
    "from single_cell_tools import *\n",
    "from preprocess import read_dataset, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a532a6-d597-4cef-bb06-68648c1a72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModifiedscDeepCluster\n",
    "import model\n",
    "import model_single_cell_tools\n",
    "import model_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60af52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c7d42d4030>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for repeatability\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc9d56",
   "metadata": {},
   "source": [
    "Setup parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e62117",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter setting\n",
    "'''\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.n_clusters = 8\n",
    "        self.knn = 20\n",
    "        self.resolution = 0.8\n",
    "        self.select_genes = 0\n",
    "        self.batch_size = 256\n",
    "        self.data_file = '10X_PBMC.h5'\n",
    "        self.maxiter = 2000\n",
    "        self.pretrain_epochs = 300\n",
    "        self.gamma = 1.\n",
    "        self.sigma = 2.5\n",
    "        self.update_interval = 1\n",
    "        self.tol = 0.001\n",
    "        self.use_attention = True\n",
    "        self.use_layernorm = False\n",
    "        self.use_batchnorm = True\n",
    "        # self.ae_weights = None\n",
    "        self.ae_weights = '10xPBMC_with_attn_plus_batch_weights.pth.tar'\n",
    "        self.save_dir = 'results/EnhancedscDeepCluster/'\n",
    "        self.ae_weight_file = '10xPBMC_with_attn_plus_batch_weights.pth.tar'\n",
    "        self.final_latent_file = '10xPBMC_final_latent_file.txt'\n",
    "        self.predict_label_file = '10xPBMC_pred_labels.txt'\n",
    "        self.device = 'cuda'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590d2b",
   "metadata": {},
   "source": [
    "Normalizating and preprocessing count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1ae96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 16653 genes and 4271 cells.\n",
      "<__main__.Args object at 0x000001C7DB9D3910>\n",
      "(4271, 16653)\n",
      "(4271,)\n"
     ]
    }
   ],
   "source": [
    "data_mat = h5py.File(args.data_file)\n",
    "x = np.array(data_mat['X'])\n",
    "# y is the ground truth labels for evaluating clustering performance\n",
    "# If not existing, we skip calculating the clustering performance metrics (e.g. NMI ARI)\n",
    "if 'Y' in data_mat:\n",
    "    y = np.array(data_mat['Y'])\n",
    "else:\n",
    "    y = None\n",
    "data_mat.close()\n",
    "\n",
    "if args.select_genes > 0:\n",
    "    importantGenes = model_single_cell_tools.geneSelection(x, n=args.select_genes, plot=False)\n",
    "    x = x[:, importantGenes]\n",
    "\n",
    "# preprocessing scRNA-seq read counts matrix\n",
    "adata = sc.AnnData(x)\n",
    "if y is not None:\n",
    "    adata.obs['Group'] = y\n",
    "\n",
    "adata = model_preprocess.read_dataset(adata,\n",
    "                 transpose=False,\n",
    "                 test_split=False,\n",
    "                 copy=True)\n",
    "\n",
    "adata = model_preprocess.normalize(adata,\n",
    "                  size_factors=True,\n",
    "                  normalize_input=True,\n",
    "                  logtrans_input=True)\n",
    "\n",
    "input_size = adata.n_vars\n",
    "\n",
    "print(args)\n",
    "\n",
    "print(adata.X.shape)\n",
    "if y is not None:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827573d",
   "metadata": {},
   "source": [
    "Build scDeepCluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d2f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedscDeepCluster(\n",
      "  (dropoutLayer): Dropout(p=0.8, inplace=False)\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=16653, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (attention): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=16653, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.EnhancedscDeepCluster(input_dim=adata.n_vars, z_dim=32, use_attention=args.use_attention, use_layernorm=args.use_layernorm, use_batchnorm=args.use_batchnorm,\n",
    "            encodeLayer=[256, 64], decodeLayer=[64, 256], sigma=args.sigma, gamma=args.gamma, device=args.device)\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c272d6",
   "metadata": {},
   "source": [
    "Pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bbcfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> loading checkpoint '10xPBMC_with_attn_plus_batch_weights.pth.tar'\n",
      "Pretraining time: 0 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain epoch 196, ZINB loss: 0.22281503\n",
      "Pretrain epoch 197, ZINB loss: 0.22263191\n",
      "Pretrain epoch 198, ZINB loss: 0.22256174\n",
      "Pretrain epoch 199, ZINB loss: 0.22243141\n",
      "Pretrain epoch 200, ZINB loss: 0.22252310\n",
      "Pretrain epoch 201, ZINB loss: 0.22236248\n",
      "Pretrain epoch 202, ZINB loss: 0.22244692\n",
      "Pretrain epoch 203, ZINB loss: 0.22217898\n",
      "Pretrain epoch 204, ZINB loss: 0.22209356\n",
      "Pretrain epoch 205, ZINB loss: 0.22210407\n",
      "Pretrain epoch 206, ZINB loss: 0.22195778\n",
      "Pretrain epoch 207, ZINB loss: 0.22193105\n",
      "Pretrain epoch 208, ZINB loss: 0.22195967\n",
      "Pretrain epoch 209, ZINB loss: 0.22182915\n",
      "Pretrain epoch 210, ZINB loss: 0.22169341\n",
      "Pretrain epoch 211, ZINB loss: 0.22163370\n",
      "Pretrain epoch 212, ZINB loss: 0.22177499\n",
      "Pretrain epoch 213, ZINB loss: 0.22147643\n",
      "Pretrain epoch 214, ZINB loss: 0.22148880\n",
      "Pretrain epoch 215, ZINB loss: 0.22136244\n",
      "Pretrain epoch 216, ZINB loss: 0.22121936\n",
      "Pretrain epoch 217, ZINB loss: 0.22130410\n",
      "Pretrain epoch 218, ZINB loss: 0.22126952\n",
      "Pretrain epoch 219, ZINB loss: 0.22104641\n",
      "Pretrain epoch 220, ZINB loss: 0.22118292\n",
      "Pretrain epoch 221, ZINB loss: 0.22099563\n",
      "Pretrain epoch 222, ZINB loss: 0.22087587\n",
      "Pretrain epoch 223, ZINB loss: 0.22089438\n",
      "Pretrain epoch 224, ZINB loss: 0.22074216\n",
      "Pretrain epoch 225, ZINB loss: 0.22068737\n",
      "Pretrain epoch 226, ZINB loss: 0.22068257\n",
      "Pretrain epoch 227, ZINB loss: 0.22058894\n",
      "Pretrain epoch 228, ZINB loss: 0.22039792\n",
      "Pretrain epoch 229, ZINB loss: 0.22041764\n",
      "Pretrain epoch 230, ZINB loss: 0.22024141\n",
      "Pretrain epoch 231, ZINB loss: 0.22030703\n",
      "Pretrain epoch 232, ZINB loss: 0.22022413\n",
      "Pretrain epoch 233, ZINB loss: 0.22024772\n",
      "Pretrain epoch 234, ZINB loss: 0.22006695\n",
      "Pretrain epoch 235, ZINB loss: 0.21997589\n",
      "Pretrain epoch 236, ZINB loss: 0.22000271\n",
      "Pretrain epoch 237, ZINB loss: 0.21992193\n",
      "Pretrain epoch 238, ZINB loss: 0.21974672\n",
      "Pretrain epoch 239, ZINB loss: 0.21986109\n",
      "Pretrain epoch 240, ZINB loss: 0.21979446\n",
      "Pretrain epoch 241, ZINB loss: 0.21957877\n",
      "Pretrain epoch 242, ZINB loss: 0.21975860\n",
      "Pretrain epoch 243, ZINB loss: 0.21946437\n",
      "Pretrain epoch 244, ZINB loss: 0.21933061\n",
      "Pretrain epoch 245, ZINB loss: 0.21927085\n",
      "Pretrain epoch 246, ZINB loss: 0.21919537\n",
      "Pretrain epoch 247, ZINB loss: 0.21924410\n",
      "Pretrain epoch 248, ZINB loss: 0.21913700\n",
      "Pretrain epoch 249, ZINB loss: 0.21917329\n",
      "Pretrain epoch 250, ZINB loss: 0.21931821\n",
      "Pretrain epoch 251, ZINB loss: 0.21905606\n",
      "Pretrain epoch 252, ZINB loss: 0.21905883\n",
      "Pretrain epoch 253, ZINB loss: 0.21893270\n",
      "Pretrain epoch 254, ZINB loss: 0.21893617\n",
      "Pretrain epoch 255, ZINB loss: 0.21871363\n",
      "Pretrain epoch 256, ZINB loss: 0.21862733\n",
      "Pretrain epoch 257, ZINB loss: 0.21849339\n",
      "Pretrain epoch 258, ZINB loss: 0.21843390\n",
      "Pretrain epoch 259, ZINB loss: 0.21844238\n",
      "Pretrain epoch 260, ZINB loss: 0.21863204\n",
      "Pretrain epoch 261, ZINB loss: 0.21838971\n",
      "Pretrain epoch 262, ZINB loss: 0.21820849\n",
      "Pretrain epoch 263, ZINB loss: 0.21811830\n",
      "Pretrain epoch 264, ZINB loss: 0.21810944\n",
      "Pretrain epoch 265, ZINB loss: 0.21804571\n",
      "Pretrain epoch 266, ZINB loss: 0.21792821\n",
      "Pretrain epoch 267, ZINB loss: 0.21811300\n",
      "Pretrain epoch 268, ZINB loss: 0.21798248\n",
      "Pretrain epoch 269, ZINB loss: 0.21796048\n",
      "Pretrain epoch 270, ZINB loss: 0.21772245\n",
      "Pretrain epoch 271, ZINB loss: 0.21774004\n",
      "Pretrain epoch 272, ZINB loss: 0.21776114\n",
      "Pretrain epoch 273, ZINB loss: 0.21758871\n",
      "Pretrain epoch 274, ZINB loss: 0.21752864\n",
      "Pretrain epoch 275, ZINB loss: 0.21744584\n",
      "Pretrain epoch 276, ZINB loss: 0.21749883\n",
      "Pretrain epoch 277, ZINB loss: 0.21763060\n",
      "Pretrain epoch 278, ZINB loss: 0.21723779\n",
      "Pretrain epoch 279, ZINB loss: 0.21728777\n",
      "Pretrain epoch 280, ZINB loss: 0.21727359\n",
      "Pretrain epoch 281, ZINB loss: 0.21724647\n",
      "Pretrain epoch 282, ZINB loss: 0.21719001\n",
      "Pretrain epoch 283, ZINB loss: 0.21702635\n",
      "Pretrain epoch 284, ZINB loss: 0.21690302\n",
      "Pretrain epoch 285, ZINB loss: 0.21694075\n",
      "Pretrain epoch 286, ZINB loss: 0.21681773\n",
      "Pretrain epoch 287, ZINB loss: 0.21676968\n",
      "Pretrain epoch 288, ZINB loss: 0.21679875\n",
      "Pretrain epoch 289, ZINB loss: 0.21676224\n",
      "Pretrain epoch 290, ZINB loss: 0.21675163\n",
      "Pretrain epoch 291, ZINB loss: 0.21686860\n",
      "Pretrain epoch 292, ZINB loss: 0.21687557\n",
      "Pretrain epoch 293, ZINB loss: 0.21667668\n",
      "Pretrain epoch 294, ZINB loss: 0.21652196\n",
      "Pretrain epoch 295, ZINB loss: 0.21626951\n",
      "Pretrain epoch 296, ZINB loss: 0.21622735\n",
      "Pretrain epoch 297, ZINB loss: 0.21628966\n",
      "Pretrain epoch 298, ZINB loss: 0.21629726\n",
      "Pretrain epoch 299, ZINB loss: 0.21615532\n",
      "Pretrain epoch 300, ZINB loss: 0.21604352\n",
      "Pretraining time: 4928 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "if args.ae_weights is None:\n",
    "    model.pretrain_autoencoder(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, \n",
    "                            batch_size=args.batch_size, epochs=args.pretrain_epochs, ae_weights=args.ae_weight_file)\n",
    "else:\n",
    "    if os.path.isfile(args.ae_weights):\n",
    "        print(\"==> loading checkpoint '{}'\".format(args.ae_weights))\n",
    "        checkpoint = torch.load(args.ae_weights)\n",
    "        model.load_state_dict(checkpoint['ae_state_dict'])\n",
    "    else:\n",
    "        print(\"==> no checkpoint found at '{}'\".format(args.ae_weights))\n",
    "        raise ValueError\n",
    "\n",
    "print('Pretraining time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0ded9",
   "metadata": {},
   "source": [
    "Clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1a72f6-053d-4196-8922-f4dcacf9086b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.6992, ARI= 0.6203\n",
      "Clustering   1: NMI= 0.6992, ARI= 0.6203\n",
      "Epoch   1: Total: 0.38007347 Clustering Loss: 0.15192962 ZINB Loss: 0.22814385\n",
      "Clustering   2: NMI= 0.7140, ARI= 0.6355\n",
      "Epoch   2: Total: 0.38603994 Clustering Loss: 0.15699160 ZINB Loss: 0.22904834\n",
      "Clustering   3: NMI= 0.7189, ARI= 0.6381\n",
      "Epoch   3: Total: 0.38364877 Clustering Loss: 0.15352353 ZINB Loss: 0.23012524\n",
      "Clustering   4: NMI= 0.7162, ARI= 0.6326\n",
      "Epoch   4: Total: 0.37812518 Clustering Loss: 0.14703964 ZINB Loss: 0.23108553\n",
      "Clustering   5: NMI= 0.7193, ARI= 0.6334\n",
      "Epoch   5: Total: 0.37492155 Clustering Loss: 0.14337515 ZINB Loss: 0.23154641\n",
      "Clustering   6: NMI= 0.7264, ARI= 0.6385\n",
      "Epoch   6: Total: 0.37277631 Clustering Loss: 0.14075095 ZINB Loss: 0.23202537\n",
      "Clustering   7: NMI= 0.7245, ARI= 0.6331\n",
      "Epoch   7: Total: 0.36905593 Clustering Loss: 0.13676073 ZINB Loss: 0.23229520\n",
      "Clustering   8: NMI= 0.7297, ARI= 0.6358\n",
      "Epoch   8: Total: 0.36102018 Clustering Loss: 0.12849322 ZINB Loss: 0.23252697\n",
      "Clustering   9: NMI= 0.7277, ARI= 0.6313\n",
      "Epoch   9: Total: 0.36042149 Clustering Loss: 0.12773211 ZINB Loss: 0.23268938\n",
      "Clustering  10: NMI= 0.7308, ARI= 0.6344\n",
      "Epoch  10: Total: 0.34849617 Clustering Loss: 0.11574009 ZINB Loss: 0.23275608\n",
      "Clustering  11: NMI= 0.7304, ARI= 0.6313\n",
      "Epoch  11: Total: 0.35160375 Clustering Loss: 0.11871164 ZINB Loss: 0.23289212\n",
      "Clustering  12: NMI= 0.7323, ARI= 0.6326\n",
      "Epoch  12: Total: 0.33797792 Clustering Loss: 0.10505075 ZINB Loss: 0.23292718\n",
      "Clustering  13: NMI= 0.7305, ARI= 0.6304\n",
      "Epoch  13: Total: 0.33960458 Clustering Loss: 0.10662058 ZINB Loss: 0.23298400\n",
      "Clustering  14: NMI= 0.7324, ARI= 0.6321\n",
      "Epoch  14: Total: 0.32990287 Clustering Loss: 0.09687762 ZINB Loss: 0.23302525\n",
      "Clustering  15: NMI= 0.7317, ARI= 0.6319\n",
      "Epoch  15: Total: 0.33009640 Clustering Loss: 0.09712795 ZINB Loss: 0.23296844\n",
      "Clustering  16: NMI= 0.7328, ARI= 0.6328\n",
      "Epoch  16: Total: 0.32155028 Clustering Loss: 0.08856375 ZINB Loss: 0.23298653\n",
      "Clustering  17: NMI= 0.7319, ARI= 0.6323\n",
      "Epoch  17: Total: 0.32280740 Clustering Loss: 0.08979886 ZINB Loss: 0.23300854\n",
      "Clustering  18: NMI= 0.7329, ARI= 0.6345\n",
      "Epoch  18: Total: 0.31549128 Clustering Loss: 0.08260282 ZINB Loss: 0.23288847\n",
      "Clustering  19: NMI= 0.7338, ARI= 0.6339\n",
      "Epoch  19: Total: 0.31573049 Clustering Loss: 0.08293694 ZINB Loss: 0.23279355\n",
      "Clustering  20: NMI= 0.7338, ARI= 0.6345\n",
      "Epoch  20: Total: 0.31133495 Clustering Loss: 0.07847943 ZINB Loss: 0.23285552\n",
      "Clustering  21: NMI= 0.7335, ARI= 0.6342\n",
      "Epoch  21: Total: 0.30996355 Clustering Loss: 0.07719112 ZINB Loss: 0.23277243\n",
      "Clustering  22: NMI= 0.7339, ARI= 0.6345\n",
      "delta_label 0.000937 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 186 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b7cb3c-a13e-472d-86cc-39729f7224f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters:  13\n",
      "Clustering stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhari\\Downloads\\EnhancedscDeepCluster\\./EnhancedscDeepCluster\\model.py:230: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  size_factor = torch.tensor(size_factor, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.6835, ARI= 0.4975\n",
      "Clustering   1: NMI= 0.6987, ARI= 0.5157\n",
      "Epoch   1: Total: 0.42755987 Clustering Loss: 0.19945309 ZINB Loss: 0.22810677\n",
      "Clustering   2: NMI= 0.6979, ARI= 0.5148\n",
      "Epoch   2: Total: 0.45328117 Clustering Loss: 0.22439147 ZINB Loss: 0.22888970\n",
      "Clustering   3: NMI= 0.7047, ARI= 0.5093\n",
      "Epoch   3: Total: 0.48460409 Clustering Loss: 0.25489396 ZINB Loss: 0.22971013\n",
      "Clustering   4: NMI= 0.6972, ARI= 0.5147\n",
      "Epoch   4: Total: 0.46957667 Clustering Loss: 0.23921238 ZINB Loss: 0.23036429\n",
      "Clustering   5: NMI= 0.7034, ARI= 0.5058\n",
      "Epoch   5: Total: 0.47704123 Clustering Loss: 0.24632962 ZINB Loss: 0.23071160\n",
      "Clustering   6: NMI= 0.6956, ARI= 0.5117\n",
      "Epoch   6: Total: 0.44203363 Clustering Loss: 0.21099622 ZINB Loss: 0.23103741\n",
      "Clustering   7: NMI= 0.6996, ARI= 0.5046\n",
      "Epoch   7: Total: 0.46137652 Clustering Loss: 0.23010533 ZINB Loss: 0.23127119\n",
      "Clustering   8: NMI= 0.6956, ARI= 0.5097\n",
      "Epoch   8: Total: 0.43591298 Clustering Loss: 0.20434545 ZINB Loss: 0.23156753\n",
      "Clustering   9: NMI= 0.6989, ARI= 0.5048\n",
      "Epoch   9: Total: 0.43870744 Clustering Loss: 0.20699643 ZINB Loss: 0.23171101\n",
      "Clustering  10: NMI= 0.6979, ARI= 0.5111\n",
      "Epoch  10: Total: 0.43325135 Clustering Loss: 0.20129713 ZINB Loss: 0.23195421\n",
      "Clustering  11: NMI= 0.6986, ARI= 0.5048\n",
      "Epoch  11: Total: 0.42456645 Clustering Loss: 0.19252223 ZINB Loss: 0.23204422\n",
      "Clustering  12: NMI= 0.6982, ARI= 0.5106\n",
      "Epoch  12: Total: 0.42820336 Clustering Loss: 0.19597706 ZINB Loss: 0.23222630\n",
      "Clustering  13: NMI= 0.6986, ARI= 0.5067\n",
      "Epoch  13: Total: 0.41570254 Clustering Loss: 0.18344399 ZINB Loss: 0.23225854\n",
      "Clustering  14: NMI= 0.6986, ARI= 0.5096\n",
      "Epoch  14: Total: 0.41670128 Clustering Loss: 0.18425104 ZINB Loss: 0.23245024\n",
      "Clustering  15: NMI= 0.6978, ARI= 0.5078\n",
      "Epoch  15: Total: 0.40881774 Clustering Loss: 0.17637339 ZINB Loss: 0.23244435\n",
      "Clustering  16: NMI= 0.6984, ARI= 0.5087\n",
      "Epoch  16: Total: 0.40861271 Clustering Loss: 0.17603087 ZINB Loss: 0.23258183\n",
      "Clustering  17: NMI= 0.6992, ARI= 0.5083\n",
      "Epoch  17: Total: 0.38903441 Clustering Loss: 0.15645110 ZINB Loss: 0.23258331\n",
      "Clustering  18: NMI= 0.6990, ARI= 0.5090\n",
      "Epoch  18: Total: 0.39993885 Clustering Loss: 0.16728969 ZINB Loss: 0.23264916\n",
      "Clustering  19: NMI= 0.6983, ARI= 0.5076\n",
      "Epoch  19: Total: 0.38929805 Clustering Loss: 0.15663645 ZINB Loss: 0.23266160\n",
      "Clustering  20: NMI= 0.6989, ARI= 0.5087\n",
      "Epoch  20: Total: 0.39682862 Clustering Loss: 0.16417002 ZINB Loss: 0.23265860\n",
      "Clustering  21: NMI= 0.6983, ARI= 0.5067\n",
      "Epoch  21: Total: 0.38481931 Clustering Loss: 0.15207701 ZINB Loss: 0.23274229\n",
      "Clustering  22: NMI= 0.6985, ARI= 0.5078\n",
      "Epoch  22: Total: 0.38920274 Clustering Loss: 0.15637469 ZINB Loss: 0.23282804\n",
      "Clustering  23: NMI= 0.6983, ARI= 0.5078\n",
      "Epoch  23: Total: 0.37954963 Clustering Loss: 0.14678886 ZINB Loss: 0.23276077\n",
      "Clustering  24: NMI= 0.6983, ARI= 0.5074\n",
      "Epoch  24: Total: 0.37506417 Clustering Loss: 0.14224416 ZINB Loss: 0.23282001\n",
      "Clustering  25: NMI= 0.6982, ARI= 0.5077\n",
      "Epoch  25: Total: 0.37261916 Clustering Loss: 0.13988780 ZINB Loss: 0.23273136\n",
      "Clustering  26: NMI= 0.6986, ARI= 0.5077\n",
      "Epoch  26: Total: 0.37034739 Clustering Loss: 0.13753716 ZINB Loss: 0.23281023\n",
      "Clustering  27: NMI= 0.6982, ARI= 0.5077\n",
      "Epoch  27: Total: 0.36828283 Clustering Loss: 0.13551514 ZINB Loss: 0.23276770\n",
      "Clustering  28: NMI= 0.6985, ARI= 0.5075\n",
      "Epoch  28: Total: 0.36738172 Clustering Loss: 0.13449312 ZINB Loss: 0.23288860\n",
      "Clustering  29: NMI= 0.6983, ARI= 0.5078\n",
      "Epoch  29: Total: 0.36187924 Clustering Loss: 0.12908444 ZINB Loss: 0.23279480\n",
      "Clustering  30: NMI= 0.6983, ARI= 0.5073\n",
      "Epoch  30: Total: 0.36365281 Clustering Loss: 0.13096821 ZINB Loss: 0.23268460\n",
      "Clustering  31: NMI= 0.6983, ARI= 0.5078\n",
      "Epoch  31: Total: 0.35730106 Clustering Loss: 0.12462821 ZINB Loss: 0.23267284\n",
      "Clustering  32: NMI= 0.6985, ARI= 0.5076\n",
      "delta_label 0.000937 < tol 0.001000\n",
      "Stopping training.\n",
      "Total time: 314 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "args.n_clusters = 0\n",
    "\n",
    "if args.n_clusters > 0:\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X.astype(np.float32), size_factor=adata.obs.size_factors, n_clusters=args.n_clusters, init_centroid=None, \n",
    "                y_pred_init=None, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "else:\n",
    "    cluster_centers, n_clusters, y_pred_init = model_preprocess.louvain_init_clustering(model=model, adata=adata, knn=args.knn, resolution=args.resolution)\n",
    "    print('Estimated number of clusters: ', n_clusters)\n",
    "    y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "                y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548b5d",
   "metadata": {},
   "source": [
    "Output and save predicted labels and latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ae42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cells:\n",
      "  NMI = 0.7339\n",
      "  ARI = 0.6345\n",
      "  Silhouette Score = -0.0192\n",
      "  Calinski-Harabasz Index = 14.48\n"
     ]
    }
   ],
   "source": [
    "if y is not None:\n",
    "    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "    ss = np.round(metrics.silhouette_score(adata.X, y_pred), 5)\n",
    "    ch = np.round(metrics.calinski_harabasz_score(adata.X, y_pred), 2)\n",
    "    print('Evaluating cells:')\n",
    "    print(f'  NMI = {nmi:.4f}')\n",
    "    print(f'  ARI = {ari:.4f}')\n",
    "    print(f'  Silhouette Score = {ss:.4f}')\n",
    "    print(f'  Calinski-Harabasz Index = {ch:.2f}')\n",
    "\n",
    "final_latent = model.encodeBatch(torch.tensor(adata.X)).cpu().numpy()\n",
    "np.savetxt(args.final_latent_file, final_latent, delimiter=\",\")\n",
    "np.savetxt(args.predict_label_file, y_pred, delimiter=\",\", fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b8c19",
   "metadata": {},
   "source": [
    "Run t-SNE on latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60652b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "                    perplexity=30,\n",
    "                    initialization=\"pca\",\n",
    "                    metric=\"euclidean\",\n",
    "                    n_jobs=8,\n",
    "                    random_state=42,\n",
    "                )\n",
    "latent_tsne_2 = tsne_embedding.fit(final_latent)\n",
    "np.savetxt(\"tsne_2D.txt\", latent_tsne_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa113e8b",
   "metadata": {},
   "source": [
    "Plot 2D t-SNE of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls())\n",
    "library(ggplot2)\n",
    "\n",
    "latent_tsne <- read.table(\"tsne_2D.txt\", sep=\",\")\n",
    "colnames(latent_tsne) <- c(\"TSNE_1\", \"TSNE_2\")\n",
    "y_pred <- as.numeric(readLines(\"pred_labels.txt\"))\n",
    "y_pred <- factor(y_pred, levels=0:max(y_pred))\n",
    "\n",
    "dat <- data.frame(latent_tsne, y_pred=y_pred)\n",
    "\n",
    "ggplot(dat, aes(x=TSNE_1, y=TSNE_2, color=y_pred)) + geom_point() + theme_classic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4ae2-5110-41b5-84f1-866a5e532364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
